{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from DCP_model import DCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load point clouds\n",
    "scan_knee = o3d.io.read_point_cloud(\"pc_volumes/scan_knee_edited1.ply\")\n",
    "CT_knee = o3d.io.read_point_cloud(\"pc_volumes/CT_knee_400.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Open3D point clouds to NumPy arrays\n",
    "def o3d_to_numpy(pcd):\n",
    "    return np.asarray(pcd.points)\n",
    "\n",
    "#Ensure equal number of points\n",
    "num_points = 1024  # Adjust if needed\n",
    "scan_knee = scan_knee.farthest_point_down_sample(num_points)\n",
    "CT_knee = CT_knee.farthest_point_down_sample(num_points)\n",
    "\n",
    "scan_knee_np = o3d_to_numpy(scan_knee)\n",
    "CT_knee_np = o3d_to_numpy(CT_knee)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 3])\n",
      "torch.Size([1, 1024, 3])\n"
     ]
    }
   ],
   "source": [
    "src = torch.tensor(scan_knee_np, dtype=torch.float32).cuda().unsqueeze(0)\n",
    "tgt = torch.tensor(CT_knee_np, dtype=torch.float32).cuda().unsqueeze(0)\n",
    "print(src.size())\n",
    "print(tgt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024])\n",
      "torch.Size([1, 3, 1024])\n"
     ]
    }
   ],
   "source": [
    "src = src.transpose(1, 2)  # Shape: (1, 3, 1024)\n",
    "tgt = tgt.transpose(1, 2)  # Shape: (1, 3, 1024)\n",
    "print(src.size())\n",
    "print(tgt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.backends.cudnn.deterministic = True\\ntorch.manual_seed(42)\\ntorch.cuda.manual_seed_all(42)\\nnp.random.seed(42)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ge75wix\\AppData\\Local\\Temp\\ipykernel_18648\\1870614810.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dcp_model.load_state_dict(torch.load(dcp_checkpoint), strict=False)  # Load trained model weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DCP(\n",
       "  (emb_nn): DGCNN(\n",
       "    (conv1): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (conv5): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (pointer): Transformer(\n",
       "    (model): EncoderDecoder(\n",
       "      (encoder): Encoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (self_attn): MultiHeadedAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (feed_forward): PositionwiseFeedForward(\n",
       "              (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (norm): Sequential()\n",
       "              (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0-1): 2 x SublayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "      (decoder): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): DecoderLayer(\n",
       "            (self_attn): MultiHeadedAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (src_attn): MultiHeadedAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (feed_forward): PositionwiseFeedForward(\n",
       "              (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (norm): Sequential()\n",
       "              (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0-2): 3 x SublayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "      (src_embed): Sequential()\n",
       "      (tgt_embed): Sequential()\n",
       "      (generator): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (head): SVDHead()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained DCP model\n",
    "dcp_checkpoint = \"dcp/pretrained/dcp_v1.t7\"\n",
    "dcp_model = DCP().cuda()\n",
    "dcp_model.load_state_dict(torch.load(dcp_checkpoint), strict=False)  # Load trained model weights\n",
    "dcp_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_ab, translation_ab, rotation_ba, translation_ba = dcp_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_ab = rotation_ab.detach().cpu().numpy()\n",
    "translation_ab = translation_ab.detach().cpu().numpy()\n",
    "rotation_ba = rotation_ba.detach().cpu().numpy()\n",
    "translation_ba = translation_ba.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.67766327 -0.14930981 -0.72005486]\n",
      "  [-0.7209557   0.05804037 -0.6905461 ]\n",
      "  [ 0.1448975   0.9870856  -0.06831388]]]\n",
      "[[ 235.13379    59.301376 -324.9567  ]]\n"
     ]
    }
   ],
   "source": [
    "print(rotation_ba)\n",
    "print(translation_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Create transformation matrices\\ntransformation_ab = np.eye(4)\\ntransformation_ab[:3, :3] = rotation_ab  # Apply rotation_ab\\ntransformation_ab[:3, 3] = translation_ab.flatten()  # Apply translation_ab\\n\\ntransformation_ba = np.eye(4)\\ntransformation_ba[:3, :3] = rotation_ba  # Apply rotation_ba\\ntransformation_ba[:3, 3] = translation_ba.flatten()  # Apply translation_ba\\n\\n# Transform the point clouds\\nscan_knee_transformed = scan_knee_1.transform(transformation_ab)  # Align scan_knee to CT_knee\\nCT_knee_transformed = CT_knee_1.transform(transformation_ba)  # Align CT_knee to scan_knee\\n\\n# Color the point clouds for distinction\\nscan_knee_transformed.paint_uniform_color([1, 0, 0])  # Red for scan_knee\\nCT_knee_transformed.paint_uniform_color([0, 1, 0])  # Green for CT_knee\\n\\n# Create a visualizer\\nvis = o3d.visualization.Visualizer()\\nvis.create_window()\\n\\n# Add both transformed point clouds to the visualizer\\nvis.add_geometry(scan_knee_transformed)\\nvis.add_geometry(CT_knee_transformed)\\n\\n# Set up the camera view (optional, but ensures both point clouds are visible)\\nctr = vis.get_view_control()\\nctr.set_zoom(0.8)  # Adjust zoom level if needed\\n\\n# Run the visualizer\\nvis.run()\\nvis.destroy_window()'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the point clouds\n",
    "scan_knee_1 = o3d.io.read_point_cloud(\"pc_volumes/scan_knee_edited1.ply\")\n",
    "CT_knee_1 = o3d.io.read_point_cloud(\"pc_volumes/CT_knee_400.ply\")\n",
    "\n",
    "# Create a 4x4 transformation matrix\n",
    "transformation_matrix = np.eye(4)\n",
    "transformation_matrix[:3, :3] = rotation_ba\n",
    "transformation_matrix[:3, 3] = translation_ba.flatten()\n",
    "\n",
    "# Apply the transformation to the scan_knee point cloud\n",
    "CT_knee_transformed = CT_knee_1.transform(transformation_matrix)\n",
    "\n",
    "# Visualize both point clouds in the same window\n",
    "CT_knee_transformed.paint_uniform_color([1, 0, 0])  # Color the transformed scan_knee red\n",
    "scan_knee_1.paint_uniform_color([0, 1, 0])  # Color the CT_knee green\n",
    "\n",
    "# Create a visualizer and add both point clouds\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "\n",
    "vis.add_geometry(CT_knee_transformed)\n",
    "vis.add_geometry(scan_knee_1)\n",
    "\n",
    "#vis.capture_screen_image('testlast.png', True)\n",
    "\n",
    "\n",
    "# Run the visualizer\n",
    "vis.run()\n",
    "'''\n",
    "# Get the point clouds from the scene\n",
    "pcd_combined = o3d.geometry.PointCloud()\n",
    "pcd_combined += CT_knee_transformed\n",
    "pcd_combined += scan_knee_1\n",
    "\n",
    "# Save the point cloud\n",
    "o3d.io.write_point_cloud(\"pc_volumes/output.ply\", pcd_combined)'''\n",
    "\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
