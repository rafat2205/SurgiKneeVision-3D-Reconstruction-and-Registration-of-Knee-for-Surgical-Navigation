{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, images_dir, masks_dir, bbox_dir, transform=None):\n",
    "       \n",
    "        self.images_dir = images_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.bbox_dir = bbox_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = sorted(os.listdir(images_dir))\n",
    "        self.mask_files = sorted(os.listdir(masks_dir))\n",
    "        self.bbox_files = sorted(os.listdir(bbox_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # Get image metadata\n",
    "        image_name = self.image_files[idx]\n",
    "        image_path = os.path.join(self.images_dir, image_name)\n",
    "        \n",
    "        # Use cv2 to read image (BGR format)\n",
    "        image = cv2.imread(image_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        \n",
    "        # Get the corresponding mask file\n",
    "        mask_name = image_name.replace('.jpg', '_mask.png')  # Adjust if needed\n",
    "        mask_path = os.path.join(self.masks_dir, mask_name)\n",
    "        \n",
    "        # Use cv2 to read the mask (grayscale)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # '0' flag is for grayscale\n",
    "\n",
    "        bbox_name = image_name.replace('.jpg', '.txt')\n",
    "        bbox_path = os.path.join(self.bbox_dir, bbox_name)\n",
    "        \n",
    "        scale_x = 256 / 1024\n",
    "        scale_y = 256 / 1024\n",
    "        \n",
    "        with open(bbox_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            coords = list(map(int, line.strip().split()))\n",
    "            x1, y1, x2, y2 = coords[1:]\n",
    "\n",
    "        bboxes = [x1, y1, x2, y2]\n",
    "        \n",
    "        x1 = int(x1 * scale_x)\n",
    "        y1 = int(y1 * scale_y)\n",
    "        x2 = int(x2 * scale_x)\n",
    "        y2 = int(y2 * scale_y)\n",
    "        \n",
    "        #bboxes = [x1, y1, x2, y2]\n",
    "        \n",
    "        bboxes = torch.tensor(bboxes)\n",
    "        \n",
    "        \n",
    "        # Apply transformations to both image and mask (if provided)\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "            \n",
    "\n",
    "        return image, mask, bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "import cv2\n",
    "\n",
    "# Define simplified transformations\n",
    "def transform(image, mask):\n",
    "    # Resize image and mask to the same size (for consistency)\n",
    "    image = cv2.resize(image, (256, 256))  # Resize image to 256x256\n",
    "    mask = cv2.resize(mask, (256, 256))    # Resize mask to 256x256\n",
    "    \n",
    "    # Convert image and mask to tensor\n",
    "    image = ToTensor()(image)  # Convert image to tensor\n",
    "    mask = ToTensor()(mask)    # Convert mask to tensor\n",
    "    \n",
    "    # Normalize the image (not the mask)\n",
    "    image = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
    "    \n",
    "\n",
    "    return {\"image\": image, \"mask\": mask}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 98,   0, 794, 483]]) torch.Size([1, 4])\n",
      "tensor([[[ 98,   0, 794, 483]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 225,  508,  846, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 225,  508,  846, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 310,  548,  898, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 310,  548,  898, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 328,  450,  982, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 328,  450,  982, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[197, 355, 860, 966]]) torch.Size([1, 4])\n",
      "tensor([[[197, 355, 860, 966]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[324, 504, 921, 995]]) torch.Size([1, 4])\n",
      "tensor([[[324, 504, 921, 995]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 230,  544,  827, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 230,  544,  827, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 98, 315, 714, 828]]) torch.Size([1, 4])\n",
      "tensor([[[ 98, 315, 714, 828]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 178,  541,  841, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 178,  541,  841, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 314,  548,  874, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 314,  548,  874, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[140, 286, 823, 919]]) torch.Size([1, 4])\n",
      "tensor([[[140, 286, 823, 919]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[300, 300, 841, 871]]) torch.Size([1, 4])\n",
      "tensor([[[300, 300, 841, 871]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[446, 439, 973, 923]]) torch.Size([1, 4])\n",
      "tensor([[[446, 439, 973, 923]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 189,  766,  732, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 189,  766,  732, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[295, 255, 837, 766]]) torch.Size([1, 4])\n",
      "tensor([[[295, 255, 837, 766]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[389, 312, 992, 952]]) torch.Size([1, 4])\n",
      "tensor([[[389, 312, 992, 952]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[287, 280, 904, 877]]) torch.Size([1, 4])\n",
      "tensor([[[287, 280, 904, 877]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 586,  316, 1025,  824]]) torch.Size([1, 4])\n",
      "tensor([[[ 586,  316, 1025,  824]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 330,  459,  880, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 330,  459,  880, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[244, 224, 816, 842]]) torch.Size([1, 4])\n",
      "tensor([[[244, 224, 816, 842]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[338, 356, 847, 903]]) torch.Size([1, 4])\n",
      "tensor([[[338, 356, 847, 903]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 284,  510,  859, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 284,  510,  859, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 450,  693, 1025, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 450,  693, 1025, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[355, 359, 930, 928]]) torch.Size([1, 4])\n",
      "tensor([[[355, 359, 930, 928]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 338,  433,  946, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 338,  433,  946, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 260,  455,  810, 1015]]) torch.Size([1, 4])\n",
      "tensor([[[ 260,  455,  810, 1015]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[437, 394, 977, 935]]) torch.Size([1, 4])\n",
      "tensor([[[437, 394, 977, 935]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 94, 375, 657, 967]]) torch.Size([1, 4])\n",
      "tensor([[[ 94, 375, 657, 967]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 489,  608,  994, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 489,  608,  994, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 363,  635,  934, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 363,  635,  934, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[222, 369, 802, 925]]) torch.Size([1, 4])\n",
      "tensor([[[222, 369, 802, 925]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 132,  690,  781, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 132,  690,  781, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[206, 222, 824, 813]]) torch.Size([1, 4])\n",
      "tensor([[[206, 222, 824, 813]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 268,  500,  839, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 268,  500,  839, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 309,  462,  843, 1018]]) torch.Size([1, 4])\n",
      "tensor([[[ 309,  462,  843, 1018]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 306,  487,  913, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 306,  487,  913, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 214,  367,  755, 1001]]) torch.Size([1, 4])\n",
      "tensor([[[ 214,  367,  755, 1001]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 132,  404,  719, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 132,  404,  719, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[   0,  475,  579, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[   0,  475,  579, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[   0,  341,  649, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[   0,  341,  649, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[421, 446, 963, 964]]) torch.Size([1, 4])\n",
      "tensor([[[421, 446, 963, 964]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[247, 417, 814, 967]]) torch.Size([1, 4])\n",
      "tensor([[[247, 417, 814, 967]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[247, 346, 760, 848]]) torch.Size([1, 4])\n",
      "tensor([[[247, 346, 760, 848]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[372, 262, 999, 841]]) torch.Size([1, 4])\n",
      "tensor([[[372, 262, 999, 841]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 313,  545,  863, 1025]]) torch.Size([1, 4])\n",
      "tensor([[[ 313,  545,  863, 1025]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[132, 134, 814, 842]]) torch.Size([1, 4])\n",
      "tensor([[[132, 134, 814, 842]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[177, 301, 785, 951]]) torch.Size([1, 4])\n",
      "tensor([[[177, 301, 785, 951]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[370, 318, 893, 875]]) torch.Size([1, 4])\n",
      "tensor([[[370, 318, 893, 875]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[169, 342, 748, 875]]) torch.Size([1, 4])\n",
      "tensor([[[169, 342, 748, 875]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[ 79, 223, 690, 791]]) torch.Size([1, 4])\n",
      "tensor([[[ 79, 223, 690, 791]]]) torch.Size([1, 1, 4])\n",
      "torch.Size([1, 3, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n",
      "tensor([[189, 306, 793, 872]]) torch.Size([1, 4])\n",
      "tensor([[[189, 306, 793, 872]]]) torch.Size([1, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "images_dir = 'knee_segmentation_robo/valid/new/resized_images'\n",
    "masks_dir = 'knee_segmentation_robo/valid/new/masks'\n",
    "bbox_dir = 'knee_segmentation_robo/valid/new/bbox_coords'\n",
    "\n",
    "\n",
    "# Initialize dataset with transform\n",
    "test_dataset = CustomDataset(images_dir, masks_dir, bbox_dir, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Iterate through the DataLoader (example)\n",
    "for images, masks, bboxes in test_loader:\n",
    "    print(images.shape)  # Batch of images\n",
    "    print(masks.shape)   # Corresponding batch of masks\n",
    "    print(bboxes, bboxes.shape) \n",
    "    bbboxes = bboxes.unsqueeze(1)\n",
    "    print(bbboxes, bbboxes.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'vit_b'\n",
    "checkpoint = 'models/sam-vit-base_custom.pth'\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ge75wix\\.conda\\envs\\Thesis_RHS\\Lib\\site-packages\\segment_anything\\build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "sam_model = sam_model_registry[model_type](checkpoint=checkpoint)\n",
    "sam_model.to(device)\n",
    "sam_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import monai\n",
    "\n",
    "# Note: Hyperparameter tuning could improve performance here\n",
    "optimizer = torch.optim.AdamW(sam_model.mask_decoder.parameters(), lr=1e-5, weight_decay=0.1)\n",
    "\n",
    "loss_fn = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coefficient(pred, target, smooth=1e-5):\n",
    "    pred = pred.float()\n",
    "    target = target.float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum()\n",
    "    dice = (2 * intersection + smooth) / (union + smooth)\n",
    "    return dice\n",
    "\n",
    "def iou_score(pred, target, smooth=1e-5):\n",
    "    pred = pred.float()\n",
    "    target = target.float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    iou = (intersection + smooth) / (union + smooth)\n",
    "    return iou\n",
    "\n",
    "def precision_recall(pred, target, smooth=1e-5):\n",
    "    pred = pred.float()\n",
    "    target = target.float()\n",
    "    true_positives = (pred * target).sum()\n",
    "    false_positives = (pred * (1 - target)).sum()\n",
    "    false_negatives = ((1 - pred) * target).sum()\n",
    "    \n",
    "    precision = (true_positives + smooth) / (true_positives + false_positives + smooth)\n",
    "    recall = (true_positives + smooth) / (true_positives + false_negatives + smooth)\n",
    "    \n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2905 | Dice: 0.8171 | IoU: 0.7080 | Precision: 0.8894 | Recall: 0.7868\n"
     ]
    }
   ],
   "source": [
    "# Initialize variables to track metrics\n",
    "test_loss = 0\n",
    "test_dice = 0\n",
    "test_iou = 0\n",
    "test_precision = 0\n",
    "test_recall = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_loader):\n",
    "        imgs, msks, bbox = data\n",
    "        imgs = imgs.to(device)\n",
    "        msks = msks.to(device)\n",
    "        input_bbox = bbox.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        input_image = sam_model.preprocess(imgs)\n",
    "        image_embedding = sam_model.image_encoder(input_image)\n",
    "        sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=input_bbox,\n",
    "            masks=None,\n",
    "        )\n",
    "        low_res_masks, _ = sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "        )\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(low_res_masks, msks.float())\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Threshold predictions to binary masks\n",
    "        pred_masks = (low_res_masks > 0.5).float()\n",
    "\n",
    "        # Compute metrics\n",
    "        test_dice += dice_coefficient(pred_masks, msks)\n",
    "        test_iou += iou_score(pred_masks, msks)\n",
    "        precision, recall = precision_recall(pred_masks, msks)\n",
    "        test_precision += precision\n",
    "        test_recall += recall\n",
    "\n",
    "    # Average metrics over the test dataset\n",
    "    test_loss /= (i + 1)\n",
    "    test_dice /= (i + 1)\n",
    "    test_iou /= (i + 1)\n",
    "    test_precision /= (i + 1)\n",
    "    test_recall /= (i + 1)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.4f} | Dice: {test_dice:.4f} | IoU: {test_iou:.4f} | Precision: {test_precision:.4f} | Recall: {test_recall:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
