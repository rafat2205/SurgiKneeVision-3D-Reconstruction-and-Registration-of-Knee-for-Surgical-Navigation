{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open3d as o3d\n",
    "import numpy as np\n",
    "from DCP_model import DCP\n",
    "#from DCP_model import get_graph_feature\n",
    "from DCP.util import transform_point_cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load point clouds\n",
    "scan_knee = o3d.io.read_point_cloud(\"pc_volumes/scan_knee_edited1.ply\")\n",
    "CT_knee = o3d.io.read_point_cloud(\"pc_volumes/CT_knee_edited2.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Open3D point clouds to NumPy arrays\n",
    "def o3d_to_numpy(pcd):\n",
    "    return np.asarray(pcd.points)\n",
    "\n",
    "#Ensure equal number of points\n",
    "num_points = 1024  # Adjust if needed\n",
    "scan_knee = scan_knee.farthest_point_down_sample(num_points)\n",
    "CT_knee = CT_knee.farthest_point_down_sample(num_points)\n",
    "\n",
    "'''scan_knee.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=5.0, max_nn=30))\n",
    "CT_knee.estimate_normals(search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=5.0, max_nn=30))'''\n",
    "\n",
    "scan_knee_np = o3d_to_numpy(scan_knee)\n",
    "CT_knee_np = o3d_to_numpy(CT_knee)\n",
    "\n",
    "# Convert to PyTorch tensors (add batch dimension)\n",
    "#small_knee_tensor = torch.tensor(small_knee_np, dtype=torch.float32).cuda().unsqueeze(0)\n",
    "#big_knee_tensor = torch.tensor(big_knee_np, dtype=torch.float32).cuda().unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024, 3])\n",
      "torch.Size([1, 1024, 3])\n"
     ]
    }
   ],
   "source": [
    "src = torch.tensor(scan_knee_np, dtype=torch.float32).cuda().unsqueeze(0)\n",
    "tgt = torch.tensor(CT_knee_np, dtype=torch.float32).cuda().unsqueeze(0)\n",
    "print(src.size())\n",
    "print(tgt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1024])\n",
      "torch.Size([1, 3, 1024])\n"
     ]
    }
   ],
   "source": [
    "src = src.transpose(1, 2)  # Shape: (1, 3, 1024)\n",
    "tgt = tgt.transpose(1, 2)  # Shape: (1, 3, 1024)\n",
    "print(src.size())\n",
    "print(tgt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.backends.cudnn.deterministic = True\\ntorch.manual_seed(42)\\ntorch.cuda.manual_seed_all(42)\\nnp.random.seed(42)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''torch.backends.cudnn.deterministic = True\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ge75wix\\AppData\\Local\\Temp\\ipykernel_9380\\1870614810.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  dcp_model.load_state_dict(torch.load(dcp_checkpoint), strict=False)  # Load trained model weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DCP(\n",
       "  (emb_nn): DGCNN(\n",
       "    (conv1): Conv2d(6, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (conv3): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (conv4): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (conv5): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (pointer): Transformer(\n",
       "    (model): EncoderDecoder(\n",
       "      (encoder): Encoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): EncoderLayer(\n",
       "            (self_attn): MultiHeadedAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (feed_forward): PositionwiseFeedForward(\n",
       "              (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (norm): Sequential()\n",
       "              (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0-1): 2 x SublayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "      (decoder): Decoder(\n",
       "        (layers): ModuleList(\n",
       "          (0): DecoderLayer(\n",
       "            (self_attn): MultiHeadedAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (src_attn): MultiHeadedAttention(\n",
       "              (linears): ModuleList(\n",
       "                (0-3): 4 x Linear(in_features=512, out_features=512, bias=True)\n",
       "              )\n",
       "            )\n",
       "            (feed_forward): PositionwiseFeedForward(\n",
       "              (w_1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "              (norm): Sequential()\n",
       "              (w_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "            )\n",
       "            (sublayer): ModuleList(\n",
       "              (0-2): 3 x SublayerConnection(\n",
       "                (norm): LayerNorm()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (norm): LayerNorm()\n",
       "      )\n",
       "      (src_embed): Sequential()\n",
       "      (tgt_embed): Sequential()\n",
       "      (generator): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (head): SVDHead()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a pre-trained DCP model\n",
    "dcp_checkpoint = \"dcp/pretrained/dcp_v1.t7\"\n",
    "dcp_model = DCP().cuda()\n",
    "dcp_model.load_state_dict(torch.load(dcp_checkpoint), strict=False)  # Load trained model weights\n",
    "dcp_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_ab, translation_ab, rotation_ba, translation_ba = dcp_model(src, tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotation_ab = rotation_ab.detach().cpu().numpy()\n",
    "translation_ab = translation_ab.detach().cpu().numpy()\n",
    "rotation_ba = rotation_ba.detach().cpu().numpy()\n",
    "translation_ba = translation_ba.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 0. 1.]]]\n",
      "[[ 113.489105 -402.71616  -285.15906 ]]\n"
     ]
    }
   ],
   "source": [
    "print(rotation_ba)\n",
    "print(translation_ba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the point clouds\n",
    "scan_knee_1 = o3d.io.read_point_cloud(\"pc_volumes/scan_knee_edited1.ply\")\n",
    "CT_knee_1 = o3d.io.read_point_cloud(\"pc_volumes/CT_knee_edited2.ply\")\n",
    "\n",
    "# Create a 4x4 transformation matrix\n",
    "transformation_matrix = np.eye(4)\n",
    "transformation_matrix[:3, :3] = rotation_ba\n",
    "transformation_matrix[:3, 3] = translation_ba.flatten()\n",
    "\n",
    "# Apply the transformation to the scan_knee point cloud\n",
    "CT_knee_transformed = CT_knee_1.transform(transformation_matrix)\n",
    "\n",
    "# Visualize both point clouds in the same window\n",
    "CT_knee_transformed.paint_uniform_color([1, 0, 0])  # Color the transformed scan_knee red\n",
    "scan_knee_1.paint_uniform_color([0, 1, 0])  # Color the CT_knee green\n",
    "\n",
    "# Create a visualizer and add both point clouds\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "\n",
    "vis.add_geometry(CT_knee_transformed)\n",
    "vis.add_geometry(scan_knee_1)\n",
    "\n",
    "#vis.capture_screen_image('testlast.png', True)\n",
    "\n",
    "\n",
    "# Run the visualizer\n",
    "vis.run()\n",
    "'''\n",
    "# Get the point clouds from the scene\n",
    "pcd_combined = o3d.geometry.PointCloud()\n",
    "pcd_combined += CT_knee_transformed\n",
    "pcd_combined += scan_knee_1\n",
    "\n",
    "# Save the point cloud\n",
    "o3d.io.write_point_cloud(\"pc_volumes/output.ply\", pcd_combined)'''\n",
    "\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply initial transformation (if any)\n",
    "# For example, if you already have a rough alignment:\n",
    "# CT_knee.transform(initial_transformation)\n",
    "\n",
    "# Run Scaled ICP (SICP)\n",
    "threshold = .40  # Set a distance threshold for correspondences\n",
    "transformation_icp = o3d.pipelines.registration.registration_icp(\n",
    "    CT_knee_transformed, scan_knee_1, threshold, np.eye(4),\n",
    "    o3d.pipelines.registration.TransformationEstimationPointToPlane(),\n",
    "    o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=2000)\n",
    ")\n",
    "\n",
    "# Apply the refined transformation\n",
    "CT_knee_refined = CT_knee_transformed.transform(transformation_icp.transformation)\n",
    "\n",
    "# Visualize the results\n",
    "\n",
    "CT_knee_refined.paint_uniform_color([0, 1, 0])  # Green for CT_knee\n",
    "\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window()\n",
    "vis.add_geometry(scan_knee_1)\n",
    "vis.add_geometry(CT_knee_refined)\n",
    "vis.run()\n",
    "vis.destroy_window()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy array\n",
    "points_CT_knee = np.asarray(CT_knee_transformed.points)\n",
    "points_scan_knee = np.asarray(scan_knee_1.points)\n",
    "\n",
    "# Save as XYZ file\n",
    "np.savetxt(\"DCP_ICP_result/CT_knee_transformed.xyz\", points_CT_knee, fmt=\"%.6f\")\n",
    "np.savetxt(\"DCP_ICP_result/scan_knee_1.xyz\", points_scan_knee, fmt=\"%.6f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcd_knee = o3d.io.read_point_cloud(\"DCP_ICP_result/CT_knee_transformed.xyz\", format=\"xyz\")\n",
    "pcd_CTknee = o3d.io.read_point_cloud(\"DCP_ICP_result/scan_knee_1.xyz\", format=\"xyz\")\n",
    "\n",
    "pcd_knee.paint_uniform_color([1, 0.706, 0])\n",
    "pcd_CTknee.paint_uniform_color([0, 0.651, 0.929])\n",
    "\n",
    "pcd_combined = pcd_CTknee + pcd_knee\n",
    "# Visualize the point cloud\n",
    "#o3d.visualization.draw_geometries([pcd_CTknee, pcd_knee])\n",
    "o3d.visualization.draw_geometries([pcd_combined])\n",
    "\n",
    "o3d.io.write_point_cloud(\"DCP_ICP_result/DCP ICP Registration1.ply\", pcd_combined)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
